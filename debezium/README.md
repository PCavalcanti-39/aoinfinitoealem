# Jornada Engenharia de dados
![Desafio Engenheiro de Dados](../Desafio%20-%20Jornada%20Engenharia%20de%20Dados.png "Desafio Engenheiro de Dados")

Bem-vindo ao Mapa do Engenheiro de Dados, um programa pr√°tico e desafiador que recria situa√ß√µes reais enfrentadas por profissionais da √°rea de dados. Neste m√≥dulo, voc√™ ser√° guiado na constru√ß√£o de uma arquitetura robusta e paralela, que simula um ambiente din√¢mico e ininterrupto, tal como ocorre no mercado.

Ao longo do curso, voc√™ aprender√° os fundamentos e aplicar√° t√©cnicas avan√ßadas de modelagem de dados, desenvolvendo consultas transacionais e anal√≠ticas em SQL, alcan√ßando um n√≠vel de maestria.

# Liga Sudoers - Arquitetura de Big Data

Este reposit√≥rio visa mostrar o processo trandicional de gera√ß√£o de dados em ambiente transacionais e ETL para ambiente analiticos. Ao subir os containeres ser√£o:
  * 1 ambiente PostgreSQL com modelagem transacional, usando 3 forma normal (3FN)
  * 1 ambiente PostgreSQL com modelagem dimensional. usando star schema. 
  * 1 ambiente com Debezium para CDC.
  * 1 ambiente com Kafka para Streaming.
  * 1 ambiente com Zookeeper para apoio aos servi√ßos.
  * 1 ambiente com Airflow para orquestra√ß√£o de pipelines.
  * 1 ambiente com DBT para Transforma√ß√£o de dados.
  * 1 ambiente com Spark para processamento distribu√≠do.
  * 1 ambiente com MinIO para armazenamento distribu√≠do.
  
  
  Dentro do reposit√≥rio teremos os scripts em Python que ir√£o simular a entrada de dados:
  * liga_sudoers_historico.py - Gera dados hist√≥ricos com pedidos com data retroativas, n√£o gera novos produtos nem novos clientes. Gera 1% de dados que ser√£o considerados fraude para treinamento do modelo. 
  * liga_sudoers_streaming.py - Gera dados streamind com pedidos com data atual, gera novos clientes e registra novos pedidos. Gera 5% de dados que ser√£o considerados fraude para treinamento do modelo. 

  [V√≠deo Explicativo](https://youtu.be/Kc-mmy8eMcA)


## Perfil e responsabilidades

Perfis de profissionais e suas responsabilidades no dia a dia dos processos mostrados.
  - [Data Architect](../docs/perfis.md#data-architect) (Arquiteto de Dados) 
  - [Data Engineer](../docs/perfis.md#data-engineer)(Engenheiro de Dados)
  - [Platform Engineer](../docs/perfis.md#plataform-engineer) (Engenheiro de Plataforma)  
  - [Data Administrator](../docs/perfis.md#data-administrator) (Administrador de Dados)
  - [DevOps Engineer](../docs/perfis.md#devops-engineer) (Engenheiro de DevOps)



# Streaming

## Configurando o PostgreSQL

### Pegando os dados do Log de Transa√ß√£o (WAL)


```bash
docker exec -it postgres_oltp /bin/bash
```

```bash
psql -U sudoers -d liga_sudoers 
```

```bash
CREATE PUBLICATION pessoas_pub FOR TABLE pessoas;

CREATE USER debezium WITH REPLICATION PASSWORD 'sudoers';
GRANT SELECT ON pessoas TO debezium;
```

## Configurando o Debezium

### Cadastrando os conectores do Debezium->Kafka
```bash
docker exec -it debezium curl -s http://localhost:8083/
docker exec -it debezium curl -s http://localhost:8083/connectors
docker exec -it debezium curl -s http://localhost:8083/connector-plugins

```


```bash
docker exec -it debezium /bin/bash


cd /home

curl -i -X POST -H "Accept:application/json" -H "Content-Type:application/json" --data @debezium-postgres.json http://debezium:8083/connectors/

curl -s http://debezium:8083/connectors/postgres-connector-pessoas/status

```

### Visualizando os t√≥picos criados
```bash
kafka-topics.sh --bootstrap-server kafka:9092 --list
```

## üìå Melhor Estrat√©gia para Ingest√£o do CDC no Data Lake

A melhor estrat√©gia depende dos requisitos de lat√™ncia, complexidade e custo computacional. Abaixo, um comparativo das abordagens discutidas:

| Estrat√©gia  | Lat√™ncia  | Complexidade | Custo Computacional |
|------------|----------|-------------|----------------------|
| **Spark Structured Streaming + Delta Lake** | Baixa (quase real-time) | Alta | Alto |
| **Airflow + Batch ETL** | M√©dia (5 min ou mais) | M√©dia | M√©dio |
| **Kafka Connect + MinIO Sink** | Baixa (quase real-time) | Baixa | Baixo |

### üîπ **Resumo das Op√ß√µes**
1. **Spark Structured Streaming + Delta Lake**  
   - üèÜ Melhor para **baixa lat√™ncia** e **consist√™ncia ACID** no Data Lake.
   - üöÄ **Vantagem:** Permite **time travel** e transa√ß√µes ACID.  
   - ‚ö† **Desvantagem:** Exige mais recursos de processamento.

2. **Airflow + Batch ETL**  
   - üèóÔ∏è Melhor para **processamento em lotes**, mais f√°cil de monitorar.  
   - üöÄ **Vantagem:** **Menor custo computacional** e simples de orquestrar.  
   - ‚ö† **Desvantagem:** **Lat√™ncia maior**, n√£o ideal para real-time.

3. **Kafka Connect + MinIO Sink**  
   - üîÑ Melhor para **ingest√£o cont√≠nua sem c√≥digo adicional**.  
   - üöÄ **Vantagem:** **F√°cil de configurar** e gerenciar.  
   - ‚ö† **Desvantagem:** N√£o permite **transforma√ß√µes complexas** antes da grava√ß√£o.

### üî• **Qual escolher?**
- Se **precisa de baixa lat√™ncia** e **confiabilidade** ‚Üí **Spark Structured Streaming + Delta Lake** ‚úÖ
- Se **preferir simplicidade e menor custo** ‚Üí **Kafka Connect + MinIO Sink** ‚úÖ
- Se **lotes s√£o suficientes e quer controle via Airflow** ‚Üí **Airflow + Batch ETL** ‚úÖ


## Pegando os dados do Log de Transa√ß√£o (WAL)
Usando o Kafka Connect vamos criar um connector que tr√°s dos dados do Kafka e os envia para o Minio. 

### Cadastrando os conectores do Kafka->Minio(S3)
```bash
curl -X POST -H "Content-Type: application/json" --data @minio-sink.json http://debezium:8083/connectors

curl -s http://debezium:8083/connectors/minio-sink-connector/status
```

### Lendo os dados dos t√≥picos 


# Continue no dbt_project/README.md


## Kafka (Comandos de apoio)
```bash
curl -X DELETE http://debezium:8083/connectors/minio-sink-connector
curl -X POST http://debezium:8083/connectors/minio-sink-connector/restart
```

## Kafka l√™ o t√≥pico
```bash
docker exec -it kafka /bin/kafka-console-consumer --bootstrap-server kafka:9092 --topic liga_sudoers.public.pessoas --from-beginning
```

