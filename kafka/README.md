# Jornada Engenharia de dados
![Desafio Engenheiro de Dados](../Desafio%20-%20Jornada%20Engenharia%20de%20Dados.png "Desafio Engenheiro de Dados")

Bem-vindo ao Mapa do Engenheiro de Dados, um programa prÃ¡tico e desafiador que recria situaÃ§Ãµes reais enfrentadas por profissionais da Ã¡rea de dados. Neste mÃ³dulo, vocÃª serÃ¡ guiado na construÃ§Ã£o de uma arquitetura robusta e paralela, que simula um ambiente dinÃ¢mico e ininterrupto, tal como ocorre no mercado.

Ao longo do curso, vocÃª aprenderÃ¡ os fundamentos e aplicarÃ¡ tÃ©cnicas avanÃ§adas de modelagem de dados, desenvolvendo consultas transacionais e analÃ­ticas em SQL, alcanÃ§ando um nÃ­vel de maestria.

# Liga Sudoers - Arquitetura de Big Data

Este repositÃ³rio visa mostrar o processo trandicional de geraÃ§Ã£o de dados em ambiente transacionais e ETL para ambiente analiticos. Ao subir os containeres serÃ£o:
  * 1 ambiente PostgreSQL com modelagem transacional, usando 3 forma normal (3FN)
  * 1 ambiente PostgreSQL com modelagem dimensional. usando star schema. 
  * 1 ambiente com Debezium para CDC.
  * 1 ambiente com Kafka para Streaming.
  * 1 ambiente com Zookeeper para apoio aos serviÃ§os.
  * 1 ambiente com Airflow para orquestraÃ§Ã£o de pipelines.
  * 1 ambiente com DBT para TransformaÃ§Ã£o de dados.
  * 1 ambiente com Spark para processamento distribuÃ­do.
  * 1 ambiente com MinIO para armazenamento distribuÃ­do.
  
  
  Dentro do repositÃ³rio teremos os scripts em Python que irÃ£o simular a entrada de dados:
  * liga_sudoers_historico.py - Gera dados histÃ³ricos com pedidos com data retroativas, nÃ£o gera novos produtos nem novos clientes. Gera 1% de dados que serÃ£o considerados fraude para treinamento do modelo. 
  * liga_sudoers_streaming.py - Gera dados streamind com pedidos com data atual, gera novos clientes e registra novos pedidos. Gera 5% de dados que serÃ£o considerados fraude para treinamento do modelo. 

  [VÃ­deo Explicativo](https://youtu.be/Kc-mmy8eMcA)


## Perfil e responsabilidades

Perfis de profissionais e suas responsabilidades no dia a dia dos processos mostrados.
 - Data Architect (Arquiteto de Dados)
 - Data Engineer (Engenheiro de Dados)
 - Data Administrator (Administrador de Dados)
 - DevOps Engineer (Engenheiro de DevOps)


## ğŸ“Œ Melhor EstratÃ©gia para IngestÃ£o do CDC no Data Lake

A melhor estratÃ©gia depende dos requisitos de latÃªncia, complexidade e custo computacional. Abaixo, um comparativo das abordagens discutidas:

| EstratÃ©gia  | LatÃªncia  | Complexidade | Custo Computacional |
|------------|----------|-------------|----------------------|
| **Spark Structured Streaming + Delta Lake** | Baixa (quase real-time) | Alta | Alto |
| **Airflow + Batch ETL** | MÃ©dia (5 min ou mais) | MÃ©dia | MÃ©dio |
| **Kafka Connect + MinIO Sink** | Baixa (quase real-time) | Baixa | Baixo |

### ğŸ”¹ **Resumo das OpÃ§Ãµes**
1. **Spark Structured Streaming + Delta Lake**  
   - ğŸ† Melhor para **baixa latÃªncia** e **consistÃªncia ACID** no Data Lake.
   - ğŸš€ **Vantagem:** Permite **time travel** e transaÃ§Ãµes ACID.  
   - âš  **Desvantagem:** Exige mais recursos de processamento.

2. **Airflow + Batch ETL**  
   - ğŸ—ï¸ Melhor para **processamento em lotes**, mais fÃ¡cil de monitorar.  
   - ğŸš€ **Vantagem:** **Menor custo computacional** e simples de orquestrar.  
   - âš  **Desvantagem:** **LatÃªncia maior**, nÃ£o ideal para real-time.

3. **Kafka Connect + MinIO Sink**  
   - ğŸ”„ Melhor para **ingestÃ£o contÃ­nua sem cÃ³digo adicional**.  
   - ğŸš€ **Vantagem:** **FÃ¡cil de configurar** e gerenciar.  
   - âš  **Desvantagem:** NÃ£o permite **transformaÃ§Ãµes complexas** antes da gravaÃ§Ã£o.

### ğŸ”¥ **Qual escolher?**
- Se **precisa de baixa latÃªncia** e **confiabilidade** â†’ **Spark Structured Streaming + Delta Lake** âœ…
- Se **preferir simplicidade e menor custo** â†’ **Kafka Connect + MinIO Sink** âœ…
- Se **lotes sÃ£o suficientes e quer controle via Airflow** â†’ **Airflow + Batch ETL** âœ…

### Pegando os dados do Log de TransaÃ§Ã£o (WAL)


```bash
docker exec -it debezium /bin/bash
```

```bash
cd /home/
curl -X POST -H "Content-Type: application/json" --data @minio-sink.json http://debezium:8083/connectors
```


curl -X POST -H "Content-Type: application/json" --data '  {
    "name": "minio-sink-connector",
    "config": {
      "connector.class": "io.confluent.connect.s3.S3SinkConnector",
      "tasks.max": "1",
      "topics": "liga_sudoers.public.pessoas",
      "s3.bucket.name": "raw",
      "s3.region": "us-east-1",
      "s3.endpoint": "http://minio:9000",
      "s3.access.key": "sudoers123",
      "s3.secret.key": "sudoers1234",
      "format.class": "io.confluent.connect.s3.format.json.JsonFormat",
      "storage.class": "io.confluent.connect.s3.storage.S3Storage",
      "flush.size": "10",
      "rotate.interval.ms": "10000"
    }
  }' http://debezium:8083/connectors



## PySpark
Vamos usar o PySpark para trazer os dados 

```bash
docker exec -it spark bash
```

```bash
spark-sql \
  --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
  --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" \
  --conf "spark.hadoop.fs.s3a.access.key=sudoers123" \
  --conf "spark.hadoop.fs.s3a.secret.key=sudoers1234" \
  --conf "spark.hadoop.fs.s3a.endpoint=http://minio:9000" \
  --conf "spark.hadoop.fs.s3a.path.style.access=true" \
  --conf "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem" \
  --conf "spark.hadoop.fs.s3a.connection.ssl.enabled=false"
  --packages io.delta:delta-spark_2.12:3.2.1,io.delta:delta-storage-3.2.1
```


# Continue no dbt_project/README.md


# pySpark

from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("MinIO Test") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .config("fs.s3a.access.key", "sudoers123") \
    .config("fs.s3a.secret.key", "sudoers1234") \
    .config("fs.s3a.endpoint", "http://minio:9000") \
    .config("fs.s3a.path.style.access", "true") \
    .config("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .config("fs.s3a.connection.ssl.enabled", "false") \
    .getOrCreate()

df = spark.read.option("header", "true").csv(f"s3a://raiz/organizations/organizations.csv")

# Criar DataFrame e salvar no MinIO
data = [("Alice", 34), ("Bob", 45)]
df = spark.createDataFrame(data, ["Name", "Age"])
df.write.mode("delta").parquet("s3a://raiz/test_output-delta/")
